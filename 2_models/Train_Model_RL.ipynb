{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model\n",
    "\n",
    "In this notebook we'll train a reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"rl\"\n",
    "\n",
    "with open(\"model/model_name\", \"w\") as text_file:\n",
    "    text_file.write(model_name)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../0_data/INTC.csv\",infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "df.to_csv('local_test/test_dir/input/data/training/data_orig.csv')\n",
    "print(\"count=%s\" % len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "df[\"close\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Run Data Preparation Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Data Preparation Code\n",
    "\n",
    "In the following cell, you can modify the data preparation code or leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/data_prep_rl.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "\n",
    "prefix = '/opt/ml/'\n",
    "input_path = prefix + 'input/data/training'\n",
    "\n",
    "data_orig_file = input_path+'/data_orig.csv'\n",
    "data_file = input_path+'/data.csv'\n",
    "\n",
    "df = pd.read_csv(data_orig_file,infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(df.head())\n",
    "\n",
    "exchange = SimulatedExchange(data_frame=df, base_instrument='USD', pretransform=True)\n",
    "\n",
    "normalize_price = MinMaxNormalizer([\"open\", \"high\", \"low\", \"close\"])\n",
    "difference_all = FractionalDifference(difference_order=0.6)\n",
    "feature_pipeline = FeaturePipeline(steps=[normalize_price, difference_all])\n",
    "exchange.feature_pipeline = feature_pipeline\n",
    "\n",
    "df1=feature_pipeline.transform(exchange.data_frame)\n",
    "#df1=exchange.data_frame\n",
    "     \n",
    "df1.to_csv(data_file)\n",
    "print(df1.head()) \n",
    "print(\"count=%s\" % (len(df1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Data Preparation Locally in a Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model/data_prep_$(cat model/model_name).py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t data_prep_$(cat model/model_name) -f Dockerfile-RL .\n",
    "!docker run -v $(pwd)/local_test/test_dir:/opt/ml --rm data_prep_$(cat model/model_name) train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"local_test/test_dir/input/data/training/data.csv\",infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(\"totalCount=%s\" % len(df))\n",
    "\n",
    "trainCount=int(len(df)*0.3)\n",
    "dfTrain = df.iloc[:trainCount]\n",
    "dfTrain.to_csv('local_test/test_dir/input/data/training/data_train.csv')\n",
    "print(\"trainCount=%s\" % len(dfTrain))\n",
    "\n",
    "dfTest = df.iloc[trainCount:]\n",
    "dfTest.to_csv('local_test/test_dir/input/data/training/data_test.csv')\n",
    "print(\"testCount=%s\" % len(dfTest))\n",
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Train the Model\n",
    "\n",
    "In the following cell, you can modify the model training code or leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/model_rl.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines.common.policies import MlpLnLstmPolicy\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from tensortrade.strategies import StableBaselinesTradingStrategy\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.rewards import RiskAdjustedReturns\n",
    "from tensortrade.actions import ManagedRiskOrders\n",
    "from tensortrade.instruments import Quantity, TradingPair, EUR, USD, BTC\n",
    "from tensortrade.wallets import Wallet, Portfolio\n",
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "from tensortrade.features.stationarity import LogDifference\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features import FeaturePipeline\n",
    "        \n",
    "def train():\n",
    "    print('Starting the training.')\n",
    "    try:\n",
    "        # These are the paths to where SageMaker mounts interesting things in your\n",
    "        # container.\n",
    "        prefix = '/opt/ml/'\n",
    "\n",
    "        input_path = prefix + 'input/data/training/data_orig.csv' #train.csv'\n",
    "        test_path = prefix + 'input/data/training/data_test.csv'\n",
    "\n",
    "        output_path = os.path.join(prefix, 'output')\n",
    "        model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "        df = pd.read_csv(input_path,infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "        print(df)\n",
    "\n",
    "        WINDOW_SIZE = 1\n",
    "        PRICE_COLUMN = 'close'\n",
    "\n",
    "        normalize = MinMaxNormalizer(inplace=True)\n",
    "        difference = LogDifference(inplace=True)\n",
    "        feature_pipeline = FeaturePipeline(steps=[normalize])\n",
    "\n",
    "        action_scheme = ManagedRiskOrders(pairs=[USD/BTC])\n",
    "        reward_scheme = RiskAdjustedReturns(return_algorithm=\"sortino\")\n",
    "\n",
    "        exchange = SimulatedExchange(data_frame=df, price_column=PRICE_COLUMN, randomize_time_slices=True)\n",
    "        wallets = [(exchange, USD, 100000.0), (exchange, BTC, 0)]\n",
    "\n",
    "        portfolio = Portfolio(base_instrument=USD,wallets=wallets)\n",
    "\n",
    "        environment = TradingEnvironment(exchange=exchange,\n",
    "                                         portfolio=portfolio,\n",
    "                                         action_scheme=action_scheme,\n",
    "                                         reward_scheme=reward_scheme,\n",
    "                                         feature_pipeline=feature_pipeline,\n",
    "                                         window_size=WINDOW_SIZE,\n",
    "                                         observe_wallets=[USD, BTC])\n",
    "\n",
    "        print('Observation Data:')\n",
    "        print(environment.observation_columns)\n",
    "\n",
    "        model = PPO2\n",
    "        policy = MlpLnLstmPolicy\n",
    "        params = { \"learning_rate\": 1e-5, 'nminibatches': 1 }\n",
    "\n",
    "        strategy = StableBaselinesTradingStrategy(environment=environment,\n",
    "                                                  model=model,\n",
    "                                                  policy=policy,\n",
    "                                                  model_kwargs=params)\n",
    "\n",
    "        perf=strategy.run(steps=10000)\n",
    "        print(\"perf=%s\" % (perf))\n",
    "        \n",
    "        #save model\n",
    "        strategy.save_agent(path=os.path.join(model_path, 'model.h5'))       \n",
    "        print('Training is complete. Model saved.')\n",
    "                \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failure\n",
    "        # Reason in the DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs\n",
    "        print(\n",
    "            'Exception during training: ' + str(e) + '\\n' + trc,\n",
    "            file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Train Locally\n",
    "\n",
    "You can choose if you want to do the training locally (Option 1) or remote via SageMaker (Option 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Local ML Image\n",
    "!echo $(cat model/model_name) \n",
    "!cp model/model_$(cat model/model_name).py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t model_$(cat model/model_name) -f Dockerfile-RL .\n",
    "!docker run -v $(pwd)/local_test/test_dir:/opt/ml --rm model_$(cat model/model_name) train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Model Artifact to Strategies Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp local_test/test_dir/model/model.h5 ../strategies/model/model_$(cat model/model_name).h5\n",
    "!ls -la ../strategies/model/model_*.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Remote Training via SageMaker\n",
    "\n",
    "You can choose if you want to do the training locally (Option 1) or remote via SageMaker (Option 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy ML Image to ECS\n",
    "!./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_name=''\n",
    "with open('model/model_name', 'r') as file:\n",
    "    model_name = file.read().replace('\\n', '')\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "\n",
    "WORK_DIRECTORY = 'local_test/test_dir/input/data/training'\n",
    "prefix = 'model_'+model_name\n",
    "job_name=prefix.replace('_','-')\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix='data')\n",
    "print(data_location)\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{prefix}:latest'\n",
    "\n",
    "classifier = sage.estimator.Estimator(\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=job_name)\n",
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Model Artifact from Amazon S3 and copy it to Strategies Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Model from S3\n",
    "model_name=classifier.model_data.replace('s3://'+sess.default_bucket()+'/','')\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(sess.default_bucket())\n",
    "my_bucket.download_file(model_name,'model.tar.gz')\n",
    "!tar -xzf model.tar.gz\n",
    "!rm model.tar.gz\n",
    "!cp model.h5 ../strategies/model/model_$(cat model_name).h5\n",
    "!ls -la ../strategies/model/model_*.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
